{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1610190896641,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "dRDDmm6zb5fa",
    "outputId": "5486ff79-eb00-4669-c20c-70a13f075f26"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1610190897049,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "hVG_Db2bXZg6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "import json\n",
    "\n",
    "path = '/content/drive/MyDrive/TFG ICO/Notebooks/Tables/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPQuhAdo07yz"
   },
   "source": [
    "## Choose the number of months corresponding to the *best* training table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1610190897769,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "DFyEA3oBVm8_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_months = 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2doMw2qVm9C"
   },
   "source": [
    "***\n",
    "### Configuration for the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1610191571936,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "xOCKrHtOOFtq"
   },
   "outputs": [],
   "source": [
    "gs_dict = {\n",
    "    'logreg': {\n",
    "        'preprocess': 'standard',\n",
    "        'classifier': LogisticRegression(random_state=0, max_iter=10000),\n",
    "        'hyperparameters': [\n",
    "            {'C': [1e-4, 1e-3, 0.01, 0.1, 1, 10, 100, 1e3, 1e4, 1e5]},\n",
    "        ]\n",
    "    },\n",
    "    'knn': {\n",
    "        'preprocess': 'minmax',\n",
    "        'classifier': KNeighborsClassifier(),\n",
    "        'hyperparameters' : [\n",
    "            {\n",
    "                'n_neighbors':[30, 45, 60, 75, 90, 115, 130, 160, 190, 220, 250, 300, 350, 400, 500],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': [1, 2, 3]\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    'svm': {\n",
    "        'preprocess': 'minmax',\n",
    "        'classifier': SVC(random_state=0),\n",
    "        'hyperparameters': [\n",
    "            {\n",
    "                'C': [1e-4, 1e-3, 0.01, 0.1, 1, 10, 100, 1e3, 1e4],\n",
    "                'kernel': ['rbf', 'sigmoid'],\n",
    "                'gamma': ['auto', 'scale', 1, 2, 4, 8, 16, 32, 64, 128, 256],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'rf': {\n",
    "        'preprocess': None,\n",
    "        'classifier': RandomForestClassifier(random_state=0),\n",
    "        'hyperparameters': [{\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_features':[5, 10, 15, 20, 30, 45, 60, None, 'sqrt', 'log2'],\n",
    "            'max_depth': [None, 2, 3, 4, 5, 10, 20],\n",
    "            'min_samples_split': [2, 3, 4, 5, 10, 20, 40],\n",
    "            'min_samples_leaf': [1, 2, 3, 5, 10, 20, 40],\n",
    "            'n_estimators':[25, 50, 100, 250, 500, 650, 800]\n",
    "        }]\n",
    "     },\n",
    "     'dtree' : {\n",
    "         'preprocess': None,\n",
    "         'classifier': DecisionTreeClassifier(random_state=0),\n",
    "         'hyperparameters': [{\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "             'max_depth': [None, 2, 3, 5, 10, 20, 40, 60, 80, 100, 120, 150, 200, 250, 300, 400], \n",
    "             'min_samples_split': [2, 3, 5, 8, 10, 20, 30, 40, 60],\n",
    "             'min_samples_leaf': [1, 2, 3, 5, 7, 10, 15, 20, 40]\n",
    "          }]\n",
    "     },\n",
    "    'xgb': {\n",
    "        'preprocess': None,\n",
    "        'classifier': XGBClassifier(random_state=0),\n",
    "        'hyperparameters': [{\n",
    "            'n_estimators':[50, 100, 250, 500, 750], \n",
    "            'max_depth':[2, 3, 5, 10, 15, 20, 25, 30, 40, 60],\n",
    "        }]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfpDb9cCVm9B"
   },
   "source": [
    "***\n",
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1610191577656,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "IM7Se3CbVm9B"
   },
   "outputs": [],
   "source": [
    "def load(months):\n",
    "    df_test = pd.read_csv(path + f'df_test_{months}.csv')\n",
    "    X_test = df_test.loc[:, df_test.columns != 'is_dead']\n",
    "    y_test = df_test['is_dead']\n",
    "\n",
    "    df_train = pd.read_csv(path + f'df_train_{months}.csv')\n",
    "    X_train = df_train.loc[:, df_train.columns != 'is_dead']\n",
    "    y_train = df_train['is_dead']\n",
    "    print(months, 'months')\n",
    "    print('=== Training set: \\n{} patients -- {}% dead \\n'.format(len(df_train), round(100*df_train.is_dead.mean(), 1)))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def scale_and_fillna(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_std = scaler.fit_transform(X_train)\n",
    "    X_train_std = pd.DataFrame(X_train_std, columns=X_train.columns)#.fillna(0)\n",
    "    X_test_std = scaler.transform(X_test)\n",
    "    X_test_std = pd.DataFrame(X_test_std, columns=X_test.columns)#.fillna(0)\n",
    "\n",
    "    scaler2 = MinMaxScaler()\n",
    "    X_train_minmax = scaler2.fit_transform(X_train)\n",
    "    X_train_minmax = pd.DataFrame(X_train_minmax, columns=X_train.columns)#.fillna(0)\n",
    "    X_test_minmax = scaler2.transform(X_test)\n",
    "    X_test_minmax = pd.DataFrame(X_test_minmax, columns=X_test.columns).fillna(0)\n",
    "\n",
    "    X_train = X_train.fillna(0)\n",
    "    X_test = X_test.fillna(0)\n",
    "\n",
    "    return X_train, X_test, X_train_std, X_test_std, X_train_minmax, X_test_minmax\n",
    "\n",
    "\n",
    "def upsample(X_train, y_train):\n",
    "    # Merge x and y into one df\n",
    "    df_train = X_train.copy()\n",
    "    df_train['is_dead'] = y_train.copy()\n",
    "\n",
    "    # Separate minority and majority\n",
    "    df_dead = df_train[df_train.is_dead == 1.]\n",
    "    df_alive = df_train[df_train.is_dead == 0.]\n",
    "\n",
    "    # Upsampling the minority\n",
    "    df_dead_upsampled = resample(df_dead, replace=True, n_samples=len(df_alive), random_state=0)\n",
    "\n",
    "    # New training set\n",
    "    df_upsampled = pd.concat([df_alive, df_dead_upsampled])\n",
    "\n",
    "    X_train_ups = df_upsampled.loc[:, df_upsampled.columns != 'is_dead']\n",
    "    y_train_ups = df_upsampled['is_dead']\n",
    "    print('=== After upsample: \\n{} patients -- {}% dead'.format(len(y_train_ups), round(100*y_train_ups.mean(), 1)))\n",
    "\n",
    "    return X_train_ups, y_train_ups\n",
    "\n",
    "\n",
    "def main(months):\n",
    "    X_train, y_train, X_test, y_test = load(months)\n",
    "    X_train, X_test, X_train_std, X_test_std, X_train_minmax, X_test_minmax = scale_and_fillna(X_train, X_test)\n",
    "\n",
    "    X_train_std, y_train_std = upsample(X_train_std, y_train)\n",
    "    X_train_minmax, y_train_minmax = upsample(X_train_minmax, y_train)\n",
    "    X_train, y_train = upsample(X_train, y_train)\n",
    "\n",
    "    return X_train, X_test, X_train_std, X_test_std, X_train_minmax, X_test_minmax, y_train, y_train_std, y_train_minmax, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, X_train_std, X_test_std, X_train_minmax, X_test_minmax, y_train, y_train_std, y_train_minmax, y_test = main(num_months)\n",
    "\n",
    "\n",
    "def fit(algorithm, X, y, cv=5):\n",
    "    start = time.time()\n",
    "    \n",
    "    ### Preprocess\n",
    "    preprocess = gs_dict[algorithm]['preprocess']\n",
    "    X = X_train.copy()\n",
    "    y = y_train.copy()\n",
    "    if preprocess == 'standard': \n",
    "        X = X_train_std.copy()\n",
    "        y = y_train_std.copy()\n",
    "    if preprocess == 'minmax': \n",
    "        X = X_train_minmax.copy()\n",
    "        y = y_train_minmax.copy()\n",
    "\n",
    "    ### Grid search with cross-validation    \n",
    "    gs = GridSearchCV(\n",
    "        gs_dict[algorithm]['classifier'], \n",
    "        gs_dict[algorithm]['hyperparameters'],\n",
    "        cv=cv, \n",
    "        scoring=['roc_auc','f1','accuracy', 'precision', 'recall'], \n",
    "        refit = False\n",
    "    )\n",
    "    gs.fit(X, y)    \n",
    "    df_cv_results = pd.DataFrame(gs.cv_results_)\n",
    "    df_cv_results['model'] = algorithm\n",
    "    \n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    if duration > 3600:\n",
    "      duration = duration/3600\n",
    "      unit = 'hours'\n",
    "    elif duration > 60:\n",
    "      duration = duration/60\n",
    "      unit = 'minutes'\n",
    "    else:\n",
    "      unit = 'seconds'\n",
    "    print('Fitted in {} {}'.format(round(duration, 1), unit))\n",
    "\n",
    "    cols_to_show = ['model', 'params', 'mean_test_roc_auc', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall']\n",
    "    df_results = df_cv_results[cols_to_show].round(3)\n",
    "\n",
    "    return df_results.sort_values(by=['mean_test_roc_auc', 'mean_test_f1'], ascending=False)\n",
    "\n",
    "\n",
    "# Get grid search results for all algorithms from csv files\n",
    "def get_global_results():\n",
    "    df = pd.DataFrame()\n",
    "    for filename in os.listdir(path):\n",
    "        if \"fit_results_scale1st_36\" in filename and \".csv\" in filename:\n",
    "            newdf = pd.read_csv(path + filename)\n",
    "            df = df.append(newdf, ignore_index=True)\n",
    "\n",
    "    # Change string dictionary to dictionary by loading as json\n",
    "    df['params'] = df.params.apply(lambda x: json.loads(x.replace('\\'', '\\\"').replace('None', 'null')))\n",
    "\n",
    "    return df.sort_values(by=['mean_test_roc_auc', 'mean_test_f1'], ascending=False)\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_test, y_pred_prob):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=None)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_best_models(df_results, X_train, X_test):\n",
    "  df_gs_results = pd.DataFrame()\n",
    "  df_final_results = pd.DataFrame(columns=['model', 'roc_auc', 'f1', 'accuracy', 'precision', 'recall'])\n",
    "\n",
    "  for i, alg in enumerate(df_results.model.unique()):\n",
    "    print('=====', alg, '=====')\n",
    "\n",
    "    # Detect best results for the algorithm\n",
    "    best_result_alg = df[df.model == alg].reset_index().iloc[0]\n",
    "    df_gs_results = df_gs_results.append(best_result_alg, ignore_index=True)\n",
    "    best_hyperparameters = best_result_alg['params']\n",
    "    print('Best hyperparameters:', best_hyperparameters)\n",
    "\n",
    "    if alg == 'svm':\n",
    "      best_hyperparameters.update({'probability': True})\n",
    "\n",
    "    preprocess = gs_dict[alg]['preprocess']\n",
    "    if preprocess == 'standard': \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        #gs_dict[algorithm].update({'scaler_used': scaler})\n",
    "    if preprocess == 'minmax': \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        #gs_dict[algorithm].update({'scaler_used': scaler})\n",
    "\n",
    "    try:\n",
    "        X_train = X_train.fillna(0)\n",
    "    except:\n",
    "        X_train = np.nan_to_num(X_train, nan=0)\n",
    "\n",
    "    # Select and train the best estimator\n",
    "    model = gs_dict[alg]['classifier'].set_params(**best_hyperparameters)\n",
    "    model.fit(X_train, y_train)\n",
    "    print('third')\n",
    "\n",
    "    #Make predictions on the test set\n",
    "    #scaler = gs_dict[alg].get('scaler_used')\n",
    "    if preprocess is not None:\n",
    "      X_test = scaler.transform(X_test)\n",
    "    try:\n",
    "      X_test = X_test.fillna(0)\n",
    "    except:\n",
    "      X_test = np.nan_to_num(X_test, nan=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Predicted', sum(y_pred), 'deaths out of', len(y_pred), 'patients ({}%)'.format(round(100*sum(y_pred)/len(y_pred),1)))\n",
    "\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    print('Min prob:', round(min(y_pred_prob), 3))\n",
    "    print('Max prob:', round(max(y_pred_prob), 3))\n",
    "\n",
    "    plot_roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "    ## Metrics\n",
    "    df_final_results.loc[i, 'model'] = alg\n",
    "    df_final_results.loc[i, 'roc_auc'] = round(roc_auc_score(y_test, y_pred_prob), 3)\n",
    "    df_final_results.loc[i, 'f1'] = round(f1_score(y_test, y_pred), 3)\n",
    "    df_final_results.loc[i, 'accuracy'] = round(accuracy_score(y_test, y_pred), 3)\n",
    "    df_final_results.loc[i, 'precision'] = round(precision_score(y_test, y_pred), 3)\n",
    "    df_final_results.loc[i, 'recall'] = round(recall_score(y_test, y_pred), 3)\n",
    "\n",
    "  df_final_results.to_csv(path + f'final_results_scale1st_{num_months}.csv', index=False)\n",
    "  return df_gs_results, df_final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tISauIRVm9D"
   },
   "source": [
    "### Fit a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_it5iAWZ18h"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1965105,
     "status": "ok",
     "timestamp": 1610180837442,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "cD5ShLkHVm9D",
    "outputId": "1b9e0d7c-bb48-402e-f6a5-6c084e9aff42"
   },
   "outputs": [],
   "source": [
    "algorithm = 'xgb'\n",
    "\n",
    "fit_results = fit(algorithm, X_train, y_train)\n",
    "fit_results.to_csv(path + f'fit_results_scale1st_{num_months}_{algorithm}.csv', index=False)\n",
    "fit_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dJY7zIA4w6t"
   },
   "source": [
    "# Choose the best model for each algorithm and evaluate with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 54421,
     "status": "ok",
     "timestamp": 1610175058361,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "3Ibw7ZNtVm9F",
    "outputId": "65d408f5-f08f-43d7-d96c-62b55da66e11"
   },
   "outputs": [],
   "source": [
    "# Use this if loading fit results from different files\n",
    "df = get_global_results()\n",
    "\n",
    "gs_results, test_results = evaluate_best_models(df, X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5R2FbOe_Q6P"
   },
   "source": [
    "### Metrics on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 46294,
     "status": "ok",
     "timestamp": 1610123327171,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "h1vuFZvG7T-u",
    "outputId": "bebc95bf-83a9-4904-8bd1-0bb292ce7fc9"
   },
   "outputs": [],
   "source": [
    "test_results.sort_values(by=['roc_auc', 'f1'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGI8m9u2_G2F"
   },
   "source": [
    "### Metrics from cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "executionInfo": {
     "elapsed": 44919,
     "status": "ok",
     "timestamp": 1610123327172,
     "user": {
      "displayName": "Mar Jovani Albalat",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghj9WFx1bAtMg06_h7i7dShczW_2A7-SCoYDCRF=s64",
      "userId": "02812456591977251408"
     },
     "user_tz": -60
    },
    "id": "a8yQMUTREga3",
    "outputId": "f3f947c7-3773-4c76-e0d5-aa112a732e69"
   },
   "outputs": [],
   "source": [
    "names_dict = {'mean_test_roc_auc': 'roc_auc', 'mean_test_f1': 'f1', 'mean_test_precision': 'precision', 'mean_test_recall': 'recall'}\n",
    "gs_results = gs_results.rename(columns=names_dict)\n",
    "cols_order = ['model', 'roc_auc', 'f1', 'precision', 'recall', 'params']\n",
    "gs_results[cols_order].sort_values(by=['roc_auc', 'f1'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj98yYnM9Z5G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3. Classification C50 scaling first.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
